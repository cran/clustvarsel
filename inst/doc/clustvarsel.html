<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Luca Scrucca" />

<meta name="date" content="2018-04-09" />

<title>A quick tour of clustvarsel</title>






<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#header {
text-align: center;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; }  code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>

</head>

<body>




<h1 class="title toc-ignore">A quick tour of <strong>clustvarsel</strong></h1>
<h4 class="author"><em>Luca Scrucca</em></h4>
<h4 class="date"><em>09 Apr 2018</em></h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#simulated-clustering-data-example">Simulated clustering data example</a></li>
<li><a href="#simulated-no-clustering-data-example">Simulated no-clustering data example</a></li>
</ul>
</div>

<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>The <strong>clustvarsel</strong> package implements variable selection methodology for Gaussian model-based clustering which allows to find the (locally) optimal subset of variables in a dataset that have group/cluster information. A greedy or headlong search can be used, either in a forward-backward or backward-forward direction, with or without sub-sampling at the hierarchical clustering stage for starting <strong>mclust</strong> models. By default the algorithm uses a sequential search, but parallelisation is also available.</p>
<p>This document gives a quick tour of <code>clustvarsel</code> (version 2.3.2) functionalities. It was written in R Markdown, using the <a href="https://cran.r-project.org/package=knitr">knitr</a> package for production. See <code>help(package=&quot;clustvarsel&quot;)</code> for further details and references provided by <code>citation(&quot;clustvarsel&quot;)</code>.</p>
<pre><code>library(clustvarsel)</code></pre>
</div>
<div id="simulated-clustering-data-example" class="section level2">
<h2>Simulated clustering data example</h2>
<p>In this example we simulate a dataset on five dimensions with only the first two variables contain clustering information, the third being highly correlated with the first one, and the remaining features which are simply noise variables.</p>
<pre><code>n &lt;- 200      # sample size
pro &lt;- 0.5    # mixing proportion
mu1 &lt;- c(0,0) # mean vector for the first cluster
mu2 &lt;- c(3,3) # mean vector for the second cluster
sigma1 &lt;- matrix(c(1,0.5,0.5,1),2,2)       # covar matrix for the first cluster
sigma2 &lt;- matrix(c(1.5,-0.7,-0.7,1.5),2,2) # covar matrix for the second cluster
X &lt;- matrix(0, n, 5, dimnames = list(NULL, paste0(&quot;X&quot;, 1:5)))
set.seed(1234) # for replication
u &lt;- runif(n)
Class &lt;- ifelse(u &lt; pro, 1, 2)
X[u &lt; pro, 1:2]  &lt;- MASS::mvrnorm(sum(u &lt; pro), mu = mu1, Sigma = sigma1)
X[u &gt;= pro, 1:2] &lt;- MASS::mvrnorm(sum(u &gt;= pro), mu = mu2, Sigma = sigma2)
X[, 3] &lt;- X[, 1] + rnorm(n)
X[, 4] &lt;- rnorm(n, mean = 1.5, sd = 2)
X[, 5] &lt;- rnorm(n, mean = 2, sd = 1)</code></pre>
<pre><code>out &lt;- clustvarsel(X, verbose = TRUE)
## iter 1
## + adding step
##   Var  BICdiff Step Decision
## 1  X2 11.17931  Add Accepted
## iter 2
## + adding step
##   Var BICdiff Step Decision
## 2  X1 85.1953  Add Accepted
## iter 3 
## + adding step
## - removing step
##   Var   BICdiff   Step Decision
## 3  X3 -14.91130    Add Rejected
## 4  X1  85.19104 Remove Rejected
out
## ------------------------------------------------------ 
## Variable selection for Gaussian model-based clustering
## Stepwise (forward/backward) greedy search
## ------------------------------------------------------ 
## 
##  Variable proposed Type of step   BICclust Model G   BICdiff Decision
##                 X2          Add  -822.6398     E 2  11.17931 Accepted
##                 X1          Add -1482.8408   VEV 2  85.19530 Accepted
##                 X3          Add -2047.7064   EEV 2 -14.91130 Rejected
##                 X1       Remove  -822.6355     E 2  85.19104 Rejected
## 
## Selected subset: X2, X1</code></pre>
<pre><code>out &lt;- clustvarsel(X, direction = &quot;backward&quot;, verbose = TRUE)
## iter 1 
## - removing step
##   Var   BICdiff   Step Decision
## 1  X3 -38.09195 Remove Accepted
## iter 2 
## - removing step
##   Var   BICdiff   Step Decision
## 2  X4 -23.50212 Remove Accepted
## iter 3 
## - removing step
## + adding step
##   Var   BICdiff   Step Decision
## 3  X5 -16.25831 Remove Accepted
## 4  X3 -14.91130    Add Rejected
## iter 4 
## - removing step
## + adding step
##   Var   BICdiff   Step Decision
## 5  X1  95.55735 Remove Rejected
## 6  X3 -14.91130    Add Rejected
out
## ------------------------------------------------------ 
## Variable selection for Gaussian model-based clustering
## Stepwise (backward/forward) greedy search
## ------------------------------------------------------ 
## 
##  Variable proposed Type of step   BICclust Model G   BICdiff Decision
##                 X3       Remove -2925.4851   EVE 2 -38.09195 Accepted
##                 X4       Remove -2067.0668   EVE 2 -23.50212 Accepted
##                 X5       Remove -1482.8408   VEV 2 -16.25831 Accepted
##                 X3          Add -2047.7064   EEV 2 -14.91130 Rejected
##                 X1       Remove  -833.0019     V 2  95.55735 Rejected
##                 X3          Add -2047.7064   EEV 2 -14.91130 Rejected
## 
## Selected subset: X1, X2</code></pre>
<pre><code>out &lt;- clustvarsel(X, search = &quot;headlong&quot;, verbose = TRUE)
## iter 1
## + adding step
##   Var  BICdiff Step Decision
## 1  X2 11.17931  Add Accepted
## iter 2
## + adding step
##   Var BICdiff Step Decision
## 2  X1 85.1953  Add Accepted
## iter 3 
## + adding step
## - removing step
##   Var  BICdiff   Step Decision
## 3  X3 -14.9113    Add Rejected
## 4  X1  85.1953 Remove Rejected
out
## ------------------------------------------------------ 
## Variable selection for Gaussian model-based clustering
## Headlong (forward/backward) search
## ------------------------------------------------------ 
## 
##  Variable proposed Type of step   BICclust Model G   BICdiff Decision
##                 X2          Add  -822.6398     E 2  11.17931 Accepted
##                 X1          Add -1482.8408   VEV 2  85.19530 Accepted
##                 X3          Add -2047.7064   EEV 2 -14.91130 Rejected
##                 X1       Remove  -822.6398     E 2  85.19530 Rejected
## 
## Selected subset: X2, X1</code></pre>
</div>
<div id="simulated-no-clustering-data-example" class="section level2">
<h2>Simulated no-clustering data example</h2>
<p>In this example we simulate a dataset on ten dimensions with no clustering. It is shown that model-based clustering on all the variables yield the wrong conclusion that 2 clusters are present, but after subset selection the Gaussian finite mixture model correctly select a single cluster solution.</p>
<pre><code>n &lt;- 200
p &lt;- 10
mu &lt;- rep(0,p)
sigma1 &lt;- matrix(c(1,0.5,0.5,1),2,2)
sigma2 &lt;- matrix(c(1.5,-0.7,-0.7,1.5),2,2)
sigma &lt;- Matrix::bdiag(sigma1, sigma2, diag(6))
set.seed(12345)
X &lt;- MASS::mvrnorm(n, mu, sigma)
colnames(X) &lt;- paste0(&quot;X&quot;, 1:p)</code></pre>
<p>Model-based clustering on all the available variables:</p>
<pre><code>mod &lt;- Mclust(X)
summary(mod$BIC)
## Best BIC values:
##              EII,2       VII,2       EII,3
## BIC      -5899.073 -5901.88582 -5922.53452
## BIC diff     0.000    -2.81261   -23.46131
summary(mod)
## ----------------------------------------------------
## Gaussian finite mixture model fitted by EM algorithm 
## ----------------------------------------------------
## 
## Mclust EII (spherical, equal volume) model with 2 components:
## 
##  log.likelihood   n df       BIC       ICL
##       -2891.255 200 22 -5899.073 -5953.953
## 
## Clustering table:
##   1   2 
##  90 110</code></pre>
<p>Subset selection using forward/backward greedy algorithm:</p>
<pre><code>(out1 &lt;- clustvarsel(X))
## ------------------------------------------------------ 
## Variable selection for Gaussian model-based clustering
## Stepwise (forward/backward) greedy search
## ------------------------------------------------------ 
## 
##  Variable proposed Type of step   BICclust Model G    BICdiff Decision
##                 X3          Add  -666.9232     E 2 -4.8472740 Accepted
##                 X7          Add -1242.8998   EII 2  1.5678544 Accepted
##                X10          Add -1814.8848   EII 2  1.5172709 Accepted
##                X10       Remove -1242.8998   EII 2  1.5172709 Rejected
##                 X6          Add -2390.3224   EII 2  0.6539224 Accepted
##                 X6       Remove -1814.8848   EII 2  0.6539224 Rejected
##                 X2          Add -2942.2744   EII 2  0.1214452 Accepted
##                 X2       Remove -2390.3224   EII 2  0.1214452 Rejected
##                 X8          Add -3495.9758   EII 2  0.1104619 Accepted
##                 X8       Remove -2942.2744   EII 2  0.1104619 Rejected
##                 X9          Add -4081.9212   EII 2 -2.0296117 Rejected
##                 X8       Remove -2942.2744   EII 2  0.1104619 Rejected
## 
## Selected subset: X3, X7, X10, X6, X2, X8</code></pre>
<pre><code>mod1 &lt;- Mclust(X[,out1$subset])
summary(mod1)
## ----------------------------------------------------
## Gaussian finite mixture model fitted by EM algorithm 
## ----------------------------------------------------
## 
## Mclust XII (spherical multivariate normal) model with 1 component:
## 
##  log.likelihood   n df       BIC       ICL
##       -1727.188 200  7 -3491.465 -3491.465
## 
## Clustering table:
##   1 
## 200</code></pre>
<p>Note that the final clustering model shown in the <code>clustvarsel</code> output is EII with 2 mixture components. However, this model has been constrained to have <span class="math inline">\(G \ge 2\)</span> components because it must be a clustering model. When the final model is fitted on the selected variables without imposing the constraint on <span class="math inline">\(G\)</span>, the BIC correctly indicates a single component model.</p>
<p>Subset selection using backward/forward greedy algorithm:</p>
<pre><code>(out2 &lt;- clustvarsel(X, direction = &quot;backward&quot;))
## ------------------------------------------------------ 
## Variable selection for Gaussian model-based clustering
## Stepwise (backward/forward) greedy search
## ------------------------------------------------------ 
## 
##  Variable proposed Type of step  BICclust Model G     BICdiff Decision
##                 X2       Remove -5346.204   EII 2 -48.0289667 Accepted
##                 X4       Remove -4696.525   EII 2  -7.1909995 Accepted
##                 X3       Remove -4032.114   EII 2  -2.3352781 Accepted
##                 X3          Add -4696.525   EII 2  -2.3352781 Rejected
##                 X7       Remove -3453.786   EII 2  -1.1967844 Accepted
##                 X7          Add -4032.114   EII 2  -1.1967844 Rejected
##                 X8       Remove -2899.545   EII 2  -0.4288443 Accepted
##                 X8          Add -3453.786   EII 2  -0.4288443 Rejected
##                 X5       Remove -2308.785   EII 2   1.0712786 Rejected
##                 X8          Add -3453.786   EII 2  -0.4288443 Rejected
## 
## Selected subset: X1, X5, X6, X9, X10</code></pre>
<pre><code>mod2 &lt;- Mclust(X[,out2$subset])
summary(mod2)
## ----------------------------------------------------
## Gaussian finite mixture model fitted by EM algorithm 
## ----------------------------------------------------
## 
## Mclust XII (spherical multivariate normal) model with 1 component:
## 
##  log.likelihood   n df       BIC       ICL
##       -1424.072 200  6 -2879.934 -2879.934
## 
## Clustering table:
##   1 
## 200</code></pre>
<p>Although the selected subset of variables is different, the same comments outlined previously apply here too.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
